{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "import kubeflow.fairing.utils\n",
    "\n",
    "from kfp.components import InputPath, OutputPath\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import pickle\n",
    "\n",
    "from kale.common.serveutils import serve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "NAMESPACE = kubeflow.fairing.utils.get_current_k8s_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hatterasuser'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class TextCleaner:\n",
    "    \"\"\"TextCleaner METHODS:\n",
    "    TextCleaner.alpha_iterator():\n",
    "    TextCleaner.alpha_iterator(text) ==>  Returns lower-case letters stripped of\n",
    "    punctuation and numbers.\n",
    "    By default alpha_iterator removes numbers and emoticons.To keep numbers:\n",
    "        cleaner.alpha_iterator(text, remove_numeric = False)\n",
    "        Likewise, to keep emoticons:\n",
    "        cleaner.alpha_iterator(text, remove_emoticon = False)\n",
    "        To Keep both:\n",
    "        cleaner.alpha_iterator(text, remove_emoticon = False, remove_numeric = False)\n",
    "\n",
    "    TextCleaner.stop_word_iterator():\n",
    "    TextCleaner.stop_word_iterator(text) ==>  Removes common \"stop\" words, like \"and\".\n",
    "\n",
    "    TextCleaner.custom_stop_word_iterator():\n",
    "    TextCleaner.custom_stop_word_iterator(text, stop_words) ==> Custom stop_words in\n",
    "    list format. stop_words are words to be removed.\n",
    "    can use this in-lieu of stop_word_iterator, or in addition to. Alternatively, you can add words\n",
    "    TextCleaner.stop_words list object, using TextCleaner.stop_words.append(word). This is the\n",
    "    preferred method if you are using the in-built stopwords.\n",
    "\n",
    "    TextCleaner.streaming_cleaner_and_tokenizer():\n",
    "    TextCleaner.streaming_cleaner_and_tokenizer ==> A text cleaner, tokenizer, and stop-word remover\n",
    "    that is designed to be used with bag-of-word, out-of-processor, or other algorithms\n",
    "    that have an argument for a tokenizer.\n",
    "    Method is applied for each row in a list. Pass this as an argument wherever you need a\n",
    "    tokenizer/processor for algorithms like bag-of-words. Note that this method keeps emoticons,\n",
    "    since they can help in determining polarity of text.\n",
    "\n",
    "     STATIC METHODS:\n",
    "        TextCleaner.tokenizer(text) ==> Returns tokens (unigrams) from a\n",
    "        list of sentences.\n",
    "\n",
    "    GENERAL USAGE:\n",
    "    Import:\n",
    "        2) from  TextCleaner2000.TextCleaner import TextCleaner\n",
    "        Instance Instantiation:\n",
    "        3) Simply  instantiate a cleaner object with empty call:\n",
    "        cleaner = TextCleaner()\n",
    "\n",
    "    CLASS METHOD USAGE:\n",
    "        For the following examples, text refers to an array-like object.\n",
    "        For best results, pass text as a list() or a Pandas\n",
    "         DataFrame column: (assuming data_frame is a pandas DataFrame) data_frame[\"column_name\"].\n",
    "        For stop words used in custom stop word removal, pass stop words as a list().\n",
    "    GENERAL NUMBER AND PUNCTUATION REMOVAL:\n",
    "        alpha_words = cleaner.alpha_iterator(text, remove_emoticon = True, remove_numeric = True)\n",
    "    COMMON STOPWORD REMOVAL:\n",
    "        cleaned_of_stops = cleaner.stop_word_iterator(text)\n",
    "    CUSTOM STOPWORD REMOVAL:\n",
    "        cleaned_of_custom_stops = cleaner.custom_stop_word_iterator(text, stop_words)\n",
    "        Remember that stop_words is a comma-separated list().\n",
    "    STREAMING TEXT:\n",
    "        Pass TextCleaner.streaming_cleaner_and_tokenizer as an argument to an algorithm\n",
    "        that calls for a tokenizer. For example, with HashingVectorizer which can\n",
    "        be used with SGDClassifier:\n",
    "            HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21,\n",
    "                        preprocessor = None,\n",
    "                        tokenizer = cleaner.streaming_cleaner_and_tokenizer)\"\"\"\n",
    "    \n",
    "    URL = \"http://minio-service.kubeflow.svc.cluster.local:9000/hatterasuser/data/stops.pkl\" #minio server\n",
    "\n",
    "    STOPWORDS = pickle.load(urllib.request.urlopen(URL))\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.emoticons = None\n",
    "\n",
    "\n",
    "    @classmethod \n",
    "    def __alphaizer(self, text, remove_numeric, remove_emoticon):\n",
    "        \"\"\"Given a string (text), removes all punctuation and numbers.\n",
    "        Returns lower-case words. Called by the iterator method\n",
    "        alpha_iterator to apply this to lists, or array-like (pandas dataframe)\n",
    "        objects.\"\"\"\n",
    "\n",
    "        if remove_numeric and remove_emoticon:\n",
    "            non_numeric = ''.join(i for i in text if not i.isdigit())\n",
    "            cleaned = re.sub('[^A-Za-z0-9]+', ' ', non_numeric)\n",
    "            return cleaned.lower().strip()\n",
    "\n",
    "        if not remove_numeric and remove_emoticon:\n",
    "            cleaned = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "            return cleaned.lower().strip()\n",
    "\n",
    "        if not remove_numeric and not remove_emoticon:\n",
    "\n",
    "            emoticons = TextCleaner.__emoticon_finder(text)\n",
    "            emoticons.replace('-', '')\n",
    "            self.set_emoticons(emoticons)\n",
    "            cleaned = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "            clean = cleaned.lower().strip() + ' ' + emoticons\n",
    "            return clean\n",
    "\n",
    "        if remove_numeric and not remove_emoticon:\n",
    "            emoticons = TextCleaner.__emoticon_finder(text)\n",
    "            emoticons.replace('-', '')\n",
    "            self.set_emoticons(emoticons)\n",
    "            \n",
    "            non_numeric = ''.join(i for i in text if not i.isdigit())\n",
    "            cleaned = re.sub('[^A-Za-z0-9]+', ' ', non_numeric)\n",
    "            clean = cleaned.lower().strip() + ' ' + emoticons\n",
    "            return clean\n",
    "\n",
    "    @staticmethod\n",
    "    def __tokenizer(text):\n",
    "        \"\"\"Given a sentence, splits sentence on blanks and returns a list of ngrams or tokens\"\"\"\n",
    "           \n",
    "        text = re.sub('<[^>]*>', '', text)\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "        text = re.sub('[\\W]+', ' ', text.lower())\\\n",
    "        + ' '.join(emoticons).replace('-','')\n",
    "        tokenized = [t for t in text.split() if t not in TextCleaner.STOPWORDS]\n",
    "        return tokenized\n",
    "\n",
    "    @classmethod\n",
    "    def __stop_word_remover(self, text, stop):\n",
    "        \"\"\"Removes common stop-words like: \"and\", \"or\",\"but\", etc. Called by\n",
    "        stop_word_iterator to apply this to lists, or array-like (pandas dataframe)\n",
    "        objects. \"\"\"\n",
    "\n",
    "        clean = ''\n",
    "        tokens = (t for t in self.__tokenizer(text) if t not in stop)\n",
    "        for t in list(tokens):\n",
    "            clean += \" \" + t\n",
    "        return clean.lstrip()\n",
    "\n",
    "    @staticmethod\n",
    "    def __emoticon_finder(text):\n",
    "        \"\"\"Finds emoticons.\"\"\"\n",
    "\n",
    "        emoticons_ = \"\"\n",
    "        emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "        for e in emoticons:\n",
    "            emoticons_ += ' ' + e\n",
    "        return emoticons_.lstrip()\n",
    "\n",
    "    @classmethod \n",
    "    def stop_word_iterator(self, text):\n",
    "        \"\"\"Calls __stop_word_remover to apply this method to array-like objects.\n",
    "        Usage: TextCleaner.stop_word_iterator(Text).\"\"\"\n",
    "\n",
    "        clean = (self.__stop_word_remover(t, TextCleaner.STOPWORDS) for t in text)\n",
    "        return list(clean)\n",
    "\n",
    "    @classmethod \n",
    "    def alpha_iterator(self, text, remove_numeric=True, remove_emoticon=True):\n",
    "        \"\"\"Calls __alphaizer to apply this method to array-like objects. Usage:\n",
    "        TextCleaner.alphaizer(Text).\n",
    "        Note: By default this method removes numbers from each string.\n",
    "        To change this behavior pass the flag remove_numerals:\n",
    "        alphaizer(Text, remove_numerals = False)\n",
    "        \"\"\"\n",
    "\n",
    "        clean = (self.__alphaizer(t, remove_numeric, remove_emoticon) for t in text)\n",
    "        return list(clean)\n",
    "    \n",
    "    @classmethod \n",
    "    def custom_stop_word_iterator(self, text, stop_words):\n",
    "        \"\"\"Removes custom stop-words. For cleaned example, \"patient\", or \"medicine\", if\n",
    "        one is dealing with medical Text and do not want to include those words\n",
    "        in analysis. Can use this method to pass any set of stop\n",
    "        words, or in-lieu of common stop-word method stop_word_iterator.Calls\n",
    "        __stop_word_remover to apply this method to array-like objects. Usage:\n",
    "        TextCleaner.custom_stop_word_iterator(Text, stop_words), where\n",
    "        stop-words and Text are in a comma-\n",
    "        separated list, or iterable.\"\"\"\n",
    "\n",
    "        clean = (self.__stop_word_remover(t, stop_words) for t in text)\n",
    "        return list(clean)\n",
    "    \n",
    "    @classmethod \n",
    "    def streaming_cleaner_and_tokenizer(self, text, remove_numeric=True, remove_emoticon=False):\n",
    "        \"\"\"Called per line of text in a a stream application, such as SGD.\n",
    "        Can be passed directly to SGD algorithms as TextCleaner.streaming_cleaner_and_tokenizer\"\"\"\n",
    "\n",
    "        alpha = self.__alphaizer(text=text, remove_numeric=remove_numeric,\n",
    "                                 remove_emoticon=remove_emoticon)\n",
    "\n",
    "        clean = self.__stop_word_remover(alpha, self.stop_words)\n",
    "        tokens = TextCleaner.tokenizer(clean)\n",
    "        return tokens\n",
    "    \n",
    "    @classmethod \n",
    "    def set_emoticons(self, emoticons):\n",
    "        \"\"\"Called from __alphaizer. \n",
    "        Sets self.emoticons value to be retrieved.\"\"\"\n",
    "\n",
    "        self.emoticons = emoticons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "block:get_data"
    ]
   },
   "outputs": [],
   "source": [
    "url = \"http://minio-service.kubeflow.svc.cluster.local:9000/hatterasuser/data/ratings_shuffled.csv\" #minio server\n",
    "\n",
    "req = requests.get(url)\n",
    "content = req.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/ratings_shuffled.csv', 'wb') as csv_file:\n",
    "\n",
    "    csv_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ratings_shuffled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First of all, I have to start this comment by ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The brilliance of this story delivers at least...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spheeris debut must be one of the best music d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have to admit that I had low expectations fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ying, a Chinese girl who speaks Czech, invited...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  First of all, I have to start this comment by ...          1\n",
       "1  The brilliance of this story delivers at least...          1\n",
       "2  Spheeris debut must be one of the best music d...          1\n",
       "3  I have to admit that I had low expectations fo...          1\n",
       "4  Ying, a Chinese girl who speaks Czech, invited...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "block:partition_data"
    ]
   },
   "outputs": [],
   "source": [
    "holdout = df.iloc[[5,6,7,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexes = [x for x in df.index if x not in holdout.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data = df.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "holdout.to_csv(\"data/holdout.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "block:clean_data",
     "prev:partition_data"
    ]
   },
   "outputs": [],
   "source": [
    "cleaner = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data['review'] = cleaner.alpha_iterator(model_data['review'], remove_emoticon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data['review'] = cleaner.stop_word_iterator(model_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start comment saying huge nightmare elm street...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brilliance story delivers skillfully crafted m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spheeris debut best music documentaries time f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>admit low expectations movie surprised enterta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ying chinese girl speaks czech invited screeni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  start comment saying huge nightmare elm street...          1\n",
       "1  brilliance story delivers skillfully crafted m...          1\n",
       "2  spheeris debut best music documentaries time f...          1\n",
       "3  admit low expectations movie surprised enterta...          1\n",
       "4  ying chinese girl speaks czech invited screeni...          1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "block:get_test_train_sets",
     "prev:clean_data"
    ]
   },
   "outputs": [],
   "source": [
    "x = model_data['review'].values\n",
    "y = model_data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "block:transform_data",
     "prev:get_test_train_sets"
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextCleaner' object has no attribute '__tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dd760ebb49e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         tokenizer = cleaner.__tokenizer())\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextCleaner' object has no attribute '__tokenizer'"
     ]
    }
   ],
   "source": [
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**20,\n",
    "                        preprocessor = None,\n",
    "                        tokenizer = cleaner.__tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtrain = vect.transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtest = vect.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = vect.transform(x) # for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "block:model_definition",
     "prev:transform_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(objective='multi:softprob')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = XGBClassifier()\n",
    "print(xgbc)\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
    "       n_estimators=100, n_jobs=1, nthread=None,\n",
    "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
    "       subsample=1, verbosity=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "block:fit_model",
     "prev:model_definition"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "block:cross_val",
     "prev:fit_model"
    ]
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(xgbc, xtrain, ytrain, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation score: 0.8022873445318826\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation score: {}\".format(scores.mean())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "kf_cv_scores = cross_val_score(xgbc, xtrain, ytrain, cv=kfold )\n",
    "print(\"K-fold CV average score: {}\".format(kf_cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:results",
     "prev:cross_val"
    ]
   },
   "outputs": [],
   "source": [
    "ypred = xgbc.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:export_model",
     "prev:results"
    ]
   },
   "outputs": [],
   "source": [
    "model = xgbc.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save file to pickle\n",
    "with open(\"model.pkl\", \"wb\") as file:\n",
    "    \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"credentials.txt\", \"r\") as f:\n",
    "    \n",
    "    creds = f.read()\n",
    "    \n",
    "aws_access_key_id = creds.split()[0]\n",
    "aws_secret_access_key = creds.split()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://10.100.97.179:9000\",\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = s3.list_buckets()\n",
    "export_bucket = \"sentiment-model\"\n",
    "export_bucket_exists = False\n",
    "\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    if bucket[\"Name\"] == export_bucket:\n",
    "        export_bucket_exists = True\n",
    "        \n",
    "if not export_bucket_exists:\n",
    "    \n",
    "    s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"sentiment-model\"\n",
    "model_version = \"1\"\n",
    "\n",
    "s3.upload_file(\"model.pkl\",\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}.pkl\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function run_pipeline_func_on_cluster in module kfp._runners:\n",
      "\n",
      "run_pipeline_func_on_cluster(pipeline_func:Callable, arguments:Mapping[str, str], run_name:str=None, experiment_name:str=None, kfp_client:kfp._client.Client=None, pipeline_conf:kfp.dsl._pipeline.PipelineConf=None)\n",
      "    Runs pipeline on KFP-enabled Kubernetes cluster.\n",
      "    \n",
      "    This command compiles the pipeline function, creates or gets an experiment\n",
      "    and submits the pipeline for execution.\n",
      "    \n",
      "    Feature stage:\n",
      "    [Alpha](https://github.com/kubeflow/pipelines/blob/07328e5094ac2981d3059314cc848fbb71437a76/docs/release/feature-stages.md#alpha)\n",
      "    \n",
      "    Args:\n",
      "      pipeline_func: A function that describes a pipeline by calling components\n",
      "      and composing them into execution graph.\n",
      "      arguments: Arguments to the pipeline function provided as a dict.\n",
      "      run_name: Optional. Name of the run to be shown in the UI.\n",
      "      experiment_name: Optional. Name of the experiment to add the run to.\n",
      "      kfp_client: Optional. An instance of kfp.Client configured for the desired\n",
      "        KFP cluster.\n",
      "      pipeline_conf: Optional. kfp.dsl.PipelineConf instance. Can specify op\n",
      "        transforms, image pull secrets and other pipeline-level configuration\n",
      "        options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(kfp.run_pipeline_func_on_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run_pipeline_func_on_cluster() missing 2 required positional arguments: 'pipeline_func' and 'arguments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ef5157a21016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline_func_on_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test-1119\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: run_pipeline_func_on_cluster() missing 2 required positional arguments: 'pipeline_func' and 'arguments'"
     ]
    }
   ],
   "source": [
    "kfp.run_pipeline_func_on_cluster(run_name=\"test-1119\", pipeline_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ping: command not found\n"
     ]
    }
   ],
   "source": [
    "minio.kubeflow.svc.cluster.local:9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hatterasdatabase : model,....,pipeline\n",
    "# need pipeline name, yaml need uuid from pipeline \n",
    "# when we dpeloy we can get pipelines for namespace -> then they can get the latest? need this to get yaml\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy querying mysql for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "johnb340/hatteras-sklearn-kale:v23",
   "experiment": {
    "id": "new",
    "name": "test"
   },
   "experiment_name": "test",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "sentiment-deploy",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
